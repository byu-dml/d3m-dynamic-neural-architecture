{
    "__init__": {
        "activation_name": "relu",
        "use_batch_norm": true,
        "use_skip": false,

        "hidden_state_size": 16,
        "lstm_n_layers": 2,
        "dropout": 0.5,

        "output_n_hidden_layers": 1,
        "output_hidden_layer_size": 8
    },
    "fit": {
        "n_epochs": 1,
        "learning_rate": 0.0001,
        "batch_size": 7,
        "drop_last": false
    },
    "predict_regression": {
        "batch_size": 2
    },
    "predict_rank": {
        "batch_size": 2
    }
}